{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "solution_set_08.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/COMP90054/2022S2_tutorials/blob/main/solution_set_09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtYhEJOLntKg"
      },
      "source": [
        "# COMP90054 AI Planning for Autonomy\n",
        "### Problem Set 09 Solution\n",
        " - Monte-Carlo tree search\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCLHR5J_0mue"
      },
      "source": [
        "## Problem 1\n",
        "\n",
        "The expansion of the tree is illustrated over the five iterations:\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1n5ruzDh9n47btzT2LViQpZcokrnpQ7JP)\n",
        "\n",
        "The process followed for the five iterations are:\n",
        "\n",
        "1. Select $(2,1)$; Expand $Up$; Do simulation from $(2,2)$; Backpropagate the reward $-1$ to $Q(2,1, Up)$. Because this is the first time the pair $(2,1), Up$ has been visited, $Q(2,1, Up) = -1$.\n",
        "\n",
        "2. Select $(2,1)$; Expand $Right$; Simulate from $(3,1)$; Backpropagate the reward $-1$. Because this is the first time the pair $(2,1), Right$ has been visited, $Q(2,1, Right) = -1$.\n",
        "\n",
        "3. Select $(2,1)$; Expand $Left$; Simulate from $(2,1$); Backpropagate the reward $1$.  Because this is the first time the pair $(2,1), Left$ has been visited, $Q(2,1, Left) = 1$.\n",
        "\n",
        "4. Select $(2,1)$; the node is fully expanded, so recursively Select $Up$ using a multi-armed bandit algorithm; Expand $Right$; Simulate from $(3,2)$; Backpropagate the reward $-1$. This is the first time the pair $(2,2), Right$ has been visited, $Q(2,2, Right) = -1$. This is the 2nd time the pair $(2,1), Up$ has been visited, so:\n",
        "\n",
        "$$\n",
        "\\begin{array}{lll}\n",
        "    Q(2,1, Up) & = & Q(2,1, Up) + \\frac{1}{N(2,1, Up)}[r + \\gamma G - Q(2,1, Up)] \\\\ \n",
        "              & = &  -1        + \\frac{1}{2}[0 + 1\\cdot -1 + 1]\\\\\n",
        "              & = & -1\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "5. Select $(2,1)$; the node is fully expanded, so recursively Select $Up$ using a multi-armed bandit algorithm; Expand $Down$; Simulate from $(2,1)$; Backpropagate the reward $1$. The backpropagation for $Q(2,2, Down)$ is straightforward. For $Q(2,1, Up)$, this is the 3rd time the pair has been visited, so:\n",
        "\n",
        "$$\n",
        "\\begin{array}{lll}\n",
        "    Q(2,1, Up) & = & Q(2,1, Up) + \\frac{1}{N(2,1, Up)}[r + \\gamma G - Q(2,1, Up)] \\\\ \n",
        "              & = &  -1        + \\frac{1}{3}[0 + 1\\cdot 1 - (-1)]\\\\\n",
        "              & = & -0.33\n",
        "\\end{array}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAVg7eLm1K8M"
      },
      "source": [
        "## Problem 2\n",
        "\n",
        "This is straightforwad. The Q-values are:\n",
        "\n",
        "$$\\begin{array}{lll}\n",
        "  Q((2,1), Up)     & = & -0.33\\\\\n",
        "  Q((2,1), Right)  & = & -1\\\\\n",
        "  Q((2,1), Left)   & = & 1\\\\\n",
        "  Q((2,1), Down)   & = & -1\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "Therefore, we would select $Left$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLm4V9UO1Oot"
      },
      "source": [
        "## Problem 3\n",
        "\n",
        "We need to calculate $\\pi$ for each of the actions based on the UCT formula and then normalise. The UCT formula is:\n",
        "\n",
        "$$\\text{argmax}_{a \\in A(s)} Q(s,a) + 2 C_p \\sqrt{\\frac{2 \\ln N(s)}{N(s,a)}}$$\n",
        "\n",
        "Let's calculate this for each of the actions:\n",
        "\n",
        "$$\\begin{aligned}\n",
        "                \\pi(s)\\ =\\ argmax_{a\\in A(s)}\n",
        "                \\begin{pmatrix}\n",
        "                Up & : &-0.33+ \\sqrt{\\frac{2\\ln{6}}{3}}\\\\\n",
        "                Right & : & -1 + \\sqrt{\\frac{2\\ln{6}}{1}}\\\\\n",
        "                Left & : & 1 + \\sqrt{\\frac{2\\ln{6}}{1}}\\\\\n",
        "                Down & : & -2 + \\sqrt{\\frac{2\\ln{6}}{1}}\\\\\n",
        "                \\end{pmatrix}\n",
        "\\end{aligned}$$ \n",
        "\n",
        "Thus we end up with:\n",
        "\n",
        "$$\\begin{aligned}\n",
        "                \\pi(s)\\ =\\ argmax_{a\\in A(s)}\n",
        "                \\begin{pmatrix}\n",
        "                Up & : &-0.33+ 1.09 = 0.76\\\\\n",
        "                Right & : & -1 + 1.89 = 0.89\\\\\n",
        "                Left & : & 1 + 1.89 = 2.89\\\\\n",
        "                Down & : & -1 + 1.89 = 0.89\\\\\n",
        "                \\end{pmatrix}\n",
        "\\end{aligned}$$ \n",
        "\n",
        "Therefore, UCT would be more likely to choose $Left$."
      ]
    }
  ]
}